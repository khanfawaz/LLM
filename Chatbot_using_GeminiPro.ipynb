{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2uUJPWGBM6vp0l9PHgfIl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanfawaz/LLM/blob/main/Chatbot_using_GeminiPro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install necessary libraries"
      ],
      "metadata": {
        "id": "0crbY7zuCPqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install the necessary libraries\n",
        "!pip install -q -U google-generativeai streamlit python-dotenv"
      ],
      "metadata": {
        "id": "1H0eW_KG-4wP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2EFbUBiS-T-6"
      },
      "outputs": [],
      "source": [
        "#Load the required libraries:\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "import os\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Gemini API:\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
      ],
      "metadata": {
        "id": "bppic8W2Dkmx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to load Gemini-Pro model and get response\n",
        "model = genai.GenerativeModel(\"gemini-pro\")\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "def get_gemini_response(question):\n",
        "\tresponse = chat.send_message(question, stream=True)\n",
        "\treturn response"
      ],
      "metadata": {
        "id": "T7o2pmSXDyFu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize Streamlit App\n",
        "st.set_page_config(page_title=\"Q&A Demo\")\n",
        "st.header(\"Gemini LLM Application\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW-xhPwVEI4Y",
        "outputId": "779f9bcd-ef8a-4e22-a743-e11b2d7c332b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-12-25 23:49:58.639 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize session_state if you need to record the chat history\n",
        "if 'chat_history' not in st.session_state:\n",
        "    st.session_state['chat_history'] = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7Y8nP7LYKUb",
        "outputId": "ba45d088-d478-4a09-aa55-36cb45c02cc6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-12-25 23:49:58.655 Session state does not function when running a script without `streamlit run`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = st.text_input(\"Input:\", key=\"input\")\n",
        "submit = st.button(\"Ask the question:\")\n",
        "\n",
        "if submit and input:\n",
        "    response = get_gemini_response(input)\n",
        "    # add user query and response to session chat history\n",
        "    st.session_state['chat_history'].append((\"You\", input))\n",
        "    st.subheader(\"The response is\")\n",
        "    for chunk in response:\n",
        "        st.write(chunk.text)\n",
        "        st.session_state['chat_history'].append((\"Bot\", chunk.text))\n",
        "st.subheader(\"The chat history is\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkPBSIk9YUlt",
        "outputId": "8b60a1f0-cf09-4ff4-efe9-e79a479bfa9d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To retrieve a chat message from the history, you can use the following code:\n",
        "for role, text in st.session_state['chat_history']:\n",
        "    st.write(f\"{role}:{text}\")"
      ],
      "metadata": {
        "id": "2ATKCx3BYYK4"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}