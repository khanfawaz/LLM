{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRR89xR4XAP5nsjiRJZCT2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanfawaz/LLM/blob/main/TextGeneration_using_GeminiPro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate text responses from text inputs"
      ],
      "metadata": {
        "id": "ezg_n_UTW2J3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the Python SDK\n",
        "\n",
        "The Python SDK for the Gemini API, is contained in the [`google-generativeai`](https://pypi.org/project/google-generativeai/) package. Install the dependency using pip:"
      ],
      "metadata": {
        "id": "Gwr5n_NPX23F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "EhAFf_-2X2LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries\n",
        "Import the necessary libraries."
      ],
      "metadata": {
        "id": "1xyK-2b-YNhW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVR-wp-mWycj"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup your API key\n",
        "\n",
        "Before you can use the Gemini API, you must first obtain an API key. If you don't already have one, create a key with one click in Google AI Studio.\n",
        "\n",
        "<a class=\"button button-primary\" href=\"https://makersuite.google.com/app/apikey\" target=\"_blank\" rel=\"noopener noreferrer\">Get an API key</a>"
      ],
      "metadata": {
        "id": "3HY5Txx8YmeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "RAIjYDMuYnV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List models\n",
        "\n",
        "Now you're ready to call the Gemini API. Use `list_models` to see the available Gemini models:\n",
        "\n",
        "* `gemini-pro`: optimized for text-only prompts.\n",
        "* `gemini-pro-vision`: optimized for text-and-images prompts."
      ],
      "metadata": {
        "id": "FLwADnMFaBGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "dKblpGLJaB6m",
        "outputId": "9f103869-bb91-459e-f7fe-ff58dc04dede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate text from text inputs\n",
        "\n",
        "For text-only prompts, use the `gemini-pro` model:"
      ],
      "metadata": {
        "id": "QN-Y94H3aLbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "id": "Tr-p6LxNaMT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `generate_content` method can handle a wide variety of use cases, including multi-turn chat and multimodal input, depending on what the underlying model supports. The available models only support text and images as input, and text as output.\n",
        "\n",
        "In the simplest case, you can pass a prompt string to the `GenerativeModel.generate_content` method:"
      ],
      "metadata": {
        "id": "vKdRBvQRaTKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "response = model.generate_content(\"How to learn Large Language Model quickly and efficiently?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "a7NYbDc7aXbv",
        "outputId": "fc196be8-4ea7-4e3e-a28d-f2b797098fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 131 ms, sys: 14.2 ms, total: 146 ms\n",
            "Wall time: 11.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In simple cases, the `response.text` accessor is all you need. To display formatted Markdown text, use the `to_markdown` function:"
      ],
      "metadata": {
        "id": "ZV7fPGftajWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6dKGuEcKamXM",
        "outputId": "989cf9b9-cb2e-47ef-a83e-2975c3d71a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> 1. **Understand the Basics First:**\n>    - Start by building a solid foundation by understanding the fundamental concepts and principles behind Large Language Models (LLMs). Read introductory articles, tutorials, and research papers to grasp the basics.\n> \n> \n> 2. **Choose a Framework or Library:**\n>    - Select a popular framework or library for LLM development, such as PyTorch, TensorFlow, or Hugging Face Transformers. Familiarize yourself with its syntax and APIs.\n> \n> \n> 3. **Explore Pre-Trained Models:**\n>    - Begin by working with pre-trained LLMs provided by platforms like Hugging Face or Google AI. Experiment with their capabilities and usage.\n> \n> \n> 4. **Diving into Fine-Tuning:**\n>    - Learn how to fine-tune pre-trained LLMs to tailor them to your specific tasks or domains. This is a crucial skill for customizing LLMs for your applications.\n> \n> \n> 5. **Understand LLM Architecture:**\n>    - Research how Transformer-based architectures, like BERT, GPT, and RoBERTa, are constructed and function.\n> \n> \n> 6. **Train Your Own LLM:**\n>    - Once you feel comfortable, try training your own LLM from scratch using a suitable dataset. It's essential to start with a smaller model and gradually scale up as your expertise grows.\n> \n> \n> 7. **Study the Mathematics Behind LLMs:**\n>    - Delve into the mathematics underpinning LLMs to understand their inner workings. This will help you troubleshoot issues more effectively.\n> \n> \n> 8. **Practice and Experiment:**\n>    - Like any skill, practice is paramount. Experiment with different tasks, datasets, and model configurations to develop a deeper understanding of LLMs.\n> \n> \n> 9. **Join Communities and Forums:**\n>    - Engage with the active LLM community by joining relevant forums, Slack groups, and online communities. Ask questions and participate in discussions to broaden your knowledge.\n> \n> \n> 10. **Contribute to Open-Source Projects:**\n>    - Consider contributing to open-source LLM projects or repositories. Working on real-world projects can provide valuable insights and practical experience.\n> \n> \n> 11. **Stay Updated with Research:**\n>    - Keep abreast of the latest advancements in LLM research by reading academic papers and following influential researchers on social media.\n> \n> \n> 12. **Test Your Skills with Competitions:**\n>    - Participate in LLM-focused competitions or challenges to showcase your skills and learn from others. These competitions provide opportunities for applying your knowledge in real-world scenarios.\n> \n> \n> 13. **Explore LLM Applications:**\n>    - Research and explore various applications of LLMs, such as text summarization, language translation, sentiment analysis, and conversational AI.\n> \n> \n> 14. **Attend Workshops or Courses:**\n>    - Consider attending specialized workshops or online courses dedicated to LLMs. These can provide structured learning experiences and in-depth insights.\n> \n> \n> 15. **Build Your Portfolio:**\n>    - Create a portfolio showcasing your LLM-related projects, code snippets, and written tutorials. A well-curated portfolio can demonstrate your skills to potential employers or collaborators."
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the API failed to return a result, use `GenerateContentRespose.prompt_feedback` to see if it was blocked due to saftey concerns regarding the prompt."
      ],
      "metadata": {
        "id": "AnpEAaHdarjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response.prompt_feedback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwWoov7BavLz",
        "outputId": "4e2279ba-305e-4fef-c2b1-8af4c5e503ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HATE_SPEECH\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HARASSMENT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
              "  probability: NEGLIGIBLE\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemini can generate multiple possible responses for a single prompt. These possible responses are called `candidates`, and you can review them to select the most suitable one as the response.\n",
        "\n",
        "View the response candidates with `GenerateContentResponse.candidates`:"
      ],
      "metadata": {
        "id": "mBnMpcyzazmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response.candidates"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLgX9duCa3Sq",
        "outputId": "8a27845f-32ee-4382-bb04-1d26423889b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[content {\n",
              "  parts {\n",
              "    text: \"Jupiter\"\n",
              "  }\n",
              "  role: \"model\"\n",
              "}\n",
              "finish_reason: STOP\n",
              "index: 0\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HATE_SPEECH\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HARASSMENT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, the model returns a response after completing the entire generation process. You can also stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated.\n",
        "\n",
        "To stream responses, use `GenerativeModel.generate_content(..., stream=True)`."
      ],
      "metadata": {
        "id": "tHysqRWra2fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "response = model.generate_content(\"How to learn Large Language Model quickly and efficiently?\", stream=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "4SmzWEaKa_KQ",
        "outputId": "a239fe11-59b3-4ced-be74-cd5d002db2a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 119 ms, sys: 21.1 ms, total: 140 ms\n",
            "Wall time: 11.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in response:\n",
        "  print(chunk.text)\n",
        "  print(\"_\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vEoXGlfbDD0",
        "outputId": "15d32794-cd12-4faf-c2e8-eaea67996bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. **Learn the Basics of Natural Language Processing (NLP)**\n",
            "\n",
            "  -\n",
            "________________________________________________________________________________\n",
            " Understand concepts like tokenization, stemming, lemmatization, stop words, and part-of-speech tagging.\n",
            "  - Familiarize yourself with different NLP\n",
            "________________________________________________________________________________\n",
            " tasks, such as text classification, named entity recognition, question answering, and machine translation.\n",
            "\n",
            "2. **Study Deep Learning Algorithms and Architectures**\n",
            "\n",
            "  - Learn about neural networks, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers.\n",
            "  - Understand the concepts of attention\n",
            "________________________________________________________________________________\n",
            " mechanisms, self-attention, and positional encoding.\n",
            "  - Familiarize yourself with pre-trained language models, such as BERT, GPT, and XLNet.\n",
            "\n",
            "3. **Set Up Your Development Environment**\n",
            "\n",
            "  - Choose a programming language and deep learning framework, such as Python and PyTorch or TensorFlow.\n",
            "  - Install the necessary libraries and packages.\n",
            "  - Set up a GPU for training large language models.\n",
            "\n",
            "4. **Get Hands-On Experience**\n",
            "\n",
            "  - Start with smaller NLP tasks, such as text classification or sentiment analysis, to gain experience working with real-world data.\n",
            "  - Work\n",
            "________________________________________________________________________________\n",
            " on Kaggle competitions or other NLP challenges to apply your skills and learn from others.\n",
            "  - Read research papers and tutorials on large language models to stay updated with the latest developments.\n",
            "\n",
            "5. **Participate in Online Courses, Workshops, and Communities**\n",
            "\n",
            "  - Enroll in online courses or workshops on large language models offered by platforms like Coursera, Udacity, or Fast.ai.\n",
            "  - Join online communities and forums dedicated to NLP and large language models, such as the NLP subreddit or the AI Stack Exchange, to ask questions, share ideas, and learn from others.\n",
            "\n",
            "6. **Build Your Own Large Language Model**\n",
            "\n",
            "  - Use pre-trained language models as a starting point and fine-tune them on your custom dataset.\n",
            "  - Experiment with different architectures, hyperparameters, and training techniques to improve the performance of your model.\n",
            "\n",
            "7. **Apply Large Language Models to Real-World Problems**\n",
            "\n",
            "  - Explore applications of large language models in fields such as natural language generation, dialogue systems, question answering, and machine translation.\n",
            "  - Develop creative projects that utilize the capabilities of large language models.\n",
            "\n",
            "8. **Stay Updated on the Latest Developments**\n",
            "\n",
            "  - Read research papers, follow industry blogs, and attend conferences\n",
            "________________________________________________________________________________\n",
            " to keep up with the latest advancements in large language models.\n",
            "  - Engage in discussions and contribute to the community by sharing your insights and experiences.\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When streaming, some response attributes are not available until you've iterated through all the response chunks. This is demonstrated below:"
      ],
      "metadata": {
        "id": "bVhu288ZbNDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"How to learn Large Language Model quickly and efficiently?\", stream=True)"
      ],
      "metadata": {
        "id": "9BvX-EFubJm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `prompt_feedback` attribute works:"
      ],
      "metadata": {
        "id": "H-ERGU11bR8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response.prompt_feedback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V_QsVf8bS31",
        "outputId": "60157f70-b13b-45d2-dd11-c9004b9abbbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HATE_SPEECH\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HARASSMENT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
              "  probability: NEGLIGIBLE\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But attributes like `text` do not:"
      ],
      "metadata": {
        "id": "y1yCZjMdbYpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  response.text\n",
        "except Exception as e:\n",
        "  print(f'{type(e).__name__}: {e}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeV7lutAbZai",
        "outputId": "812d85f0-4f26-4f9c-e579-b6621d3e5d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IncompleteIterationError: Please let the response complete iteration before accessing the final accumulated\n",
            "attributes (or call `response.resolve()`)\n"
          ]
        }
      ]
    }
  ]
}